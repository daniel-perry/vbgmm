<html>
<head>
<title>Gaussian Mixture Models using Variational Bayes</title>
<!--
////////////////////////////////////
// The MIT License
//
// Copyright (c) 2012 Daniel Perry.
//
// Permission is hereby granted, free of charge, to any person obtaining a
// copy of this software and associated documentation files (the "Software"),
// to deal in the Software without restriction, including without limitation
// the rights to use, copy, modify, merge, publish, distribute, sublicense,
// and/or sell copies of the Software, and to permit persons to whom the
// Software is furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included
// in all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
// THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
// FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
// DEALINGS IN THE SOFTWARE.
////////////////////////////////////////
-->
</head>

<!-- libs -->
<script type="text/javascript" src="LaTeXMathML.js"></script>
<script type="text/javascript" src="underscore.js"></script>
<!--
<script type="text/javascript" src="d3.v2.min.js"></script>
<script type="text/javascript" src="jquery-1.8.1.min.js"></script>
-->

<!-- local -->
<script type="text/javascript" src="kmeans.js"></script>
<script type="text/javascript" src="script.js"></script>

<!-- data sets: -->
<script type="text/javascript" src="faithful.js"></script>

<!-- css -->
<link rel="stylesheet" type="text/css" href="style.css"></link>

<body>

<div align="center" id="title" name="title" class="section">
	<h1 style="padding-top: 100px;">Gaussian Mixture Models using Variational Bayes</h1>
	<h3>by Daniel Perry</h3>
	<h4 style="padding-top: 100px;">presentation url:</h4>
	  	<p>
		 		to follow along: <a href="http://www.cs.utah.edu/~dperry/class/seminar/vbgmm">www.cs.utah.edu/~dperry/class/seminar/vbgmm</a>
			</p>
	  	<p>
				to post comments/suggestions: <a href="http://images-and-data.blogspot.com">images-and-data.blogspot.com</a>
			</p>
</div>


<div id="kmeans" name="kmeans" class="section">
	<h2>K-means clustering</h2>

	<div id="kmeans_def" class="subsection">
	Goal: cluster similar data together
	<ul>
  <li> data: N points of a D-dimensional data set, $\{x_1,...,x_N\}$, where $x_i$ refers to the i<sup>th</sup> point</li>
	<li> similarity: requires some measure of similarity, we'll use Euclidean distance.</li>
	<li> cluster membership: clusters will be described by center value, and each point will only belong to a single cluster. <br>
  	Notation:
		<ul>
		<li>$\mu_k$ describes the center of the k<sup>th</sup> cluster.</li>
		<li>membership will be denoted with a "1-of-K" vector, $r_{nk} \in \{0,1\}$ corresponding to each $x_i$.<br>$r_{ik} = 1$ and $r_{ij} = 0$ for $j\nek$ if $x_{i}$ belongs to cluster $k$.
    </li>
<!-- TODO: add visual of this? -->
	  </ul>
  </li>
	</ul>
  </div>

  <div id="kmeans_objective" class="subsection">

  <p>We can define an objective function:</p>
  <p>$J = \sum_n\sum_k r_{nk}||x_n-\mu_k||^2$ &nbsp;&nbsp;&nbsp;(1)</p>
  <p>Which corresponds to the "sum of squares" of each point to its assigned cluster center.</p>

	<p> The real goal is to minimize $J$, ie find the values of $\mu$ and $r$ that minimize $J$.</p>
	
	<p> The optimal $\mu$ and $r$ can be derived analytically from $J$, but we will only describe the derivation here.</p>

  <ul>
  <li><b>cluster center:</b><br>
  derivation: the cluster center that minimizes the sum of squares to every point in the cluster, is the mean:<br>
  $\vec{\mu_k} = (\sum_nr_{nk}\vec{x_n})/(\sum_nr_{ik})$ &nbsp;&nbsp;&nbsp;(2)
  </li>
  <li><b>cluster membership:</b><br>
  derivation: the cluster membership that minimizes sum of squares to the mean, is the cluster with the closest center: <br>
  $r_{nk} = 1 &nbsp;&nbsp;if $ $k=arg min_j ||\vec{x_n} - \vec{\mu_j}||^2$, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)<br>$r_{nk} = 0$ otherwise.
  </li>
  </ul>

	<p>Note the circular dependency above betwen $\mu_k$ and $r_{\cdot k}$ (cluster mean and cluster membership)</p>
  </div>

  <div id="kmeans_steps" class="subsection">
	<p>Because of this circular dependency, we find the optimum by iteratively switching between them, until there is no change.</p>
  <p>(note: this is guaranteed to find a local optimum, but not the global optimum)</p>
  Steps:
  <ol>
  <li> $\mu$ initialized randomly. </li>
	<li> Solve for $r$ using (3). </li>
  <li> Solve for $\mu$ using (2). </li>
  <li> Repeat steps 2 and 3, until $\mu$ and $r$ don't change. </li>
  </ol>
  </div>

	<div id="kmeans_examples" class="subsection">
	<p>Example: <a href="#" onclick="runKmeans(false); return false;">[step]</a><a href="#" onclick="runKmeans(true); return false;">[finish]</a><a href="#" onclick="restartKmeans(); return false;">[restart]</a></p>
  <table border=0>
	<tr>
	<td>
	<canvas id="kmeans_example" width="500" height="500">
	</canvas>
	</td>
	<td>
	<canvas id="kmeans_chart" width="500" height="500"></canvas>
	</td>
	</tr>
	</table>
  <script type="text/javascript">
  var kmeans = new Kmeans();
  kmeans.canvasId =  'kmeans_example';
  kmeans.canvasId2 =  'kmeans_chart';
  </script>
  </div>

</div>

<div id="gmm" name="gmm" class="section">
<a name="gmm"></a>
	<h2>Gaussian Mixture Models (GMM)</h2>
	<div id="gmm_definition" class="subsection">
		<h3>Definition</h3>
		<p> A <a href="http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian mixture distribution</a> as a linear superposition of Gaussians.</p>
		<p>we have $k$ gaussian distributions: $N(x|\mu_k,\Sigma_k)$</p>
    <p>Each point $x$:</p>
		<p> has a mixing coefficient: $\pi_k$: $0 \le \pi_k \le 1$ and $\sum_k\pi_k=1$</p>
		<p>so we can say the gaussian mixture for point $x$ is:</p>
		<p>$p(x) = \sum_k\pi_k N(x|\mu_k,\Sigma_k)$ &nbsp;&nbsp;&nbsp;(4)</p>
	</div>
	<div id="gmm_kmeans" class="subsection">
		<h3>Aside: K-means as GMM</h3>
		<p>One way of thining of K-means is fitting a GMM where means vary, but all covariances are the same and the mixing coefficient is a "1-of-K" vector.</p>
		<p> ie (hand waving, general idea):</p>
		<p>$p(x) = \sum_k\pi_k N(x|\mu_k,\Sigma)$ - note that $\Sigma$ does not vary with $k$.</p>
		<p> where $\mu_k$ corresponds to the k<sup>th</sup> cluster center,</p>
		<p>and $\pi_k = r_{\cdot k}$ for each $x$.</p>
		<p><i>This relationship can help in thinking about GMM's in general.</i></p>

	</div>
	<div id="gmm_ml" class="subsection">
		<h3>GMM Maximum Likelihood</h3>
		<p>Data set: A set of N, D-dimensional observations $\{x_1,...,x_N\}$, represented by a N x D matrix $X$.</p>
		<p><i>Goal</i>: fit a GMM to the data.</p>
		<p>To measure error (similar to $J$ for k-means), we use log-likelihood:</p>
		<p></i>(ie, what is the <a href="http://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> the GMM matches $X$ - basically a probability over the data)</i></p>
		<p>ln$ p(X|\pi,\mu,\Sigma) = \sum_n $ln$\{ \sum_k \pi_k N(x_n|\mu_k,\Sigma_k) \}$ &nbsp;&nbsp;&nbsp;(5) - compare to (1)</p>
		<p><i>Goal update</i>: to fit the GMM, we need to maximize (5).</p>
		
	</div>
</div>

<div id="em" name="em" class="section">
	<h2>Expectation-Maximization</h2>
	<div id="em_gmm" class="subsection">
	<p><i>(we will only be discussing EM in the context of GMM)</i></p>
	<p>In order to frame the EM of GMM, we need an explicit latent variable $z$:</p>
	<p>$z_{n}$ - a binary 1-of-K latent variable corresponding to sample $x_n$, where $p(z_{nk}=1)=\pi_k$</p>
	<p>Because of this, we can say $p(z) = \prod_k \pi_k^{z_k} = \pi_k$.</p>
	<p>Likewise, $p(x|z) = \prod_k N(x|\mu_k,\Sigma_k)^{z_k} = N(x|\mu_k,\Sigma_k)$</p>
	<p>Consequenty, p(x) = \sum_z p(z)p(x|z) = \sum_k \pi_kN(x|\mu_k,\Sigma_k)</p>
	<p><i>We have successfully rewritten the Mixture of Gaussians with an explicit latent variable, $z$. </i></p>
	<br>
	<p> It will also be useful to use the conditional probability of $z$ given $x$:</p>
	<p>$\gamma(z_k) = p(z_k=1|x)$</p>
	<br>
	<p>We view $\pi_k$ as the <b>prior probability</b> of $z_k=1$, and $\gamma(z_k)$ as the <b>posterior probability</b> once we have observed $x$.</p>
	</div>
	<div id="em_step" class="subsection">
	<p><i>(for brevity, I leave out the derivations, but by solving the derivatives of the log-likelihood function, we find equations used in E and M steps below.)</i></p>
	<h3>EM steps</h3>
	<ol>
	  <li>
  	Initialize $\mu,\Sigma,\pi$.<br>Evaluate initial log-likelihood.
  	</li>
  	<li>
		<b>E-step</b><br>Evaluate the responsibilities using current parameters:<br>
		$\gamma(z_{nk}) = \frac{\pi_kN(x_n|\mu_k,\Sigma_k)}{\sum_j^K \pi_j N(x_n|\mu_j,\Sigma_j)}$
    </li>
		<li>
 		<b>M-step</b><br>Re-estimate parameters using current responsibilities:<br>
		$\mu_k^{i+1} = \frac{1}{N_k}\sum_n\gamma(z_{nk})x_n$<br>
		$\Sigma_k^{i+1} = \frac{1}{N_k}\sum_n\gamma(z_{nk})(x_n-\mu_k^{i+1})(x_n-\mu_k^{i+1})^T$<br>
		$\pi_k^{i+1} = \frac{N_k}{N}$<br>
		where $N_k = \sum_n\gamma(z_{nk})
		</li>
		<li>
		Evalute log-likelihood.<br> Check for convergence.<br> If not, return to step 2.
		</li>
	</ol>
	</i>This 2-step iteration process is very similar to what we did in K-means above.</i>
	</div>
	<div id="em_example" class="subsection">
	Example:
	</div>
</div>

<div id="vb" name="vb" class="section">
	<h2>Variational Bayes</h2>
	<div id="vb_bayes" class="subsection">
		<h3>Bayes formulation</h3>
	</div>
	<div id="vb_estimation" class="subsection">
	  <h3>Variational Inference</h3>
	</div>
</div>

<div id="references" name="references" class="section">
	<h2>Presentation URL</h2>
		<ul>
	  	<li>
		 		to review: <a href="http://www.cs.utah.edu/~dperry/class/seminar/vbgmm">www.cs.utah.edu/~dperry/class/seminar/vbgmm</a>
			</li>
	  	<li>
				to post comments/suggestions: <a href="http://images-and-data.blogspot.com">images-and-data.blogspot.com</a>
			</li>
		</ul>
	<h2>References</h2>
	<ol>
		<li>
			Bishop, Christopher M. (2006). "Pattern Recognition and Machine Learning". Springer. ISBN 0-387-31073-8.
			(sections 10.2 and 9.4)
		</li>
		<li>
			<a href="http://research.microsoft.com/~cmbishop/downloads/Bishop-AIStats01.pdf" target="_blank">http://research.microsoft.com/~cmbishop/downloads/Bishop-AIStats01.pdf</a>
		</li>
		<li>
			<a href="http://www.goldenmetallic.com/research/uai99.pdf" target="_blank">http://www.goldenmetallic.com/research/uai99.pdf</a>
		</li>
		<li>
			<a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" target="_blank">https://en.wikipedia.org/wiki/Variational_Bayesian_methods</a>
		</li>
	</ol>
</div>


</body>
</html>

